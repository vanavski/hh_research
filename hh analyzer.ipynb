{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from urllib.parse import urlencode\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "import argparse\n",
    "import hashlib\n",
    "import pickle\n",
    "import nltk\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "\n",
    "# CACHE_DIR = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'cache')\n",
    "API_BASE_URL = 'https://api.hh.ru/vacancies'\n",
    "\n",
    "DEFAULT_PARAMETERS = {\n",
    "    'area': 1624, #kazan\n",
    "    'per_page': 50,\n",
    "}\n",
    "\n",
    "HH_URL = API_BASE_URL + '?' + urlencode(DEFAULT_PARAMETERS)\n",
    "\n",
    "EX_URL = 'https://api.exchangerate-api.com/v4/latest/RUB'\n",
    "MAX_WORKERS = int(os.getenv('MAX_WORKERS', 5))\n",
    "\n",
    "exchange_rates = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_exchange_rates():\n",
    "    \"\"\"\n",
    "    Parse exchange rate for RUB, USD, EUR and save them to `exchange_rates`\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print('Try to get rates from URL...')\n",
    "        resp = requests.get(EX_URL)\n",
    "        rates = resp.json()['rates']\n",
    "\n",
    "    except requests.exceptions.SSLError:\n",
    "        print('Cannot get exchange rate! Try later or change host API')\n",
    "        exit('Exit from script. Cannot get data from URL!')\n",
    "\n",
    "    for curr in ['RUB', 'USD', 'EUR']:\n",
    "        exchange_rates[curr] = rates[curr]\n",
    "\n",
    "    # Change 'RUB' to 'RUR'\n",
    "    exchange_rates['RUR'] = exchange_rates.pop('RUB')\n",
    "    print(f'Get exchange rates: {exchange_rates}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tags(str_html):\n",
    "    \"\"\"\n",
    "    Remove HTML tags from string (text)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    str_html: str\n",
    "        Input string with tags\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "        Clean text without tags\n",
    "\n",
    "    \"\"\"\n",
    "    pat = re.compile('<.*?>')\n",
    "    res = re.sub(pat, '', str_html)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vacancy(vacancy_id):\n",
    "    # Vacancy URL\n",
    "    url = f'https://api.hh.ru/vacancies/{vacancy_id}'\n",
    "    vacancy = requests.api.get(url).json()\n",
    "\n",
    "    # Extract salary\n",
    "    salary = vacancy['salary']\n",
    "\n",
    "    # Calculate salary:\n",
    "    # Get salary into {RUB, USD, EUR} with {Gross} parameter and\n",
    "    # return a new salary in RUB.\n",
    "    cl_ex = {'from': None, 'to': None}\n",
    "    if salary:\n",
    "        # fn_gr = lambda: 0.87 if vsal['gross'] else 1\n",
    "        def fn_gr():\n",
    "            return 0.87 if vacancy['salary']['gross'] else 1\n",
    "\n",
    "        for i in cl_ex:\n",
    "            if vacancy['salary'][i] is not None:\n",
    "                cl_ex[i] = int(\n",
    "                    fn_gr() * salary[i] / exchange_rates[salary['currency']]\n",
    "                )\n",
    "\n",
    "    # Create pages tuple\n",
    "    return (\n",
    "        vacancy_id,\n",
    "        vacancy['employer']['name'],\n",
    "        vacancy['name'],\n",
    "        salary is not None,\n",
    "        cl_ex['from'],\n",
    "        cl_ex['to'],\n",
    "        vacancy['experience']['name'],\n",
    "        vacancy['schedule']['name'],\n",
    "        [el['name'] for el in vacancy['key_skills']],\n",
    "        clean_tags(vacancy['description']),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vacancies(query, refresh=False):\n",
    "    \"\"\"\n",
    "    Parse vacancy JSON: get vacancy name, salary, experience etc.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query: str\n",
    "        Search query\n",
    "    refresh: bool\n",
    "        Refresh cached data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        List of useful arguments from vacancies\n",
    "\n",
    "    \"\"\"\n",
    "#     cache_hash = hashlib.md5(query.encode()).hexdigest()\n",
    "#     cache_file_name = os.path.join(CACHE_DIR, cache_hash)\n",
    "#     try:\n",
    "#         if not refresh:\n",
    "#             return pickle.load(open(cache_file_name, 'rb'))\n",
    "\n",
    "#     except (FileNotFoundError, pickle.UnpicklingError):\n",
    "#         pass\n",
    "\n",
    "    ids = []\n",
    "    parameters = {'text': query, **DEFAULT_PARAMETERS}\n",
    "    url = API_BASE_URL + '?' + urlencode(parameters)\n",
    "    nm_pages = requests.get(url).json()['pages']\n",
    "    for i in range(nm_pages + 1):\n",
    "        resp = requests.get(url, {'page': i})\n",
    "        data = resp.json()\n",
    "        if 'items' not in data:\n",
    "            break\n",
    "        ids.extend(x['id'] for x in data['items'])\n",
    "\n",
    "    vacancies = []\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        for vacancy in tqdm(executor.map(get_vacancy, ids), total=len(ids)):\n",
    "            vacancies.append(vacancy)\n",
    "\n",
    "    pickle.dump(vacancies, open('hh_data.csv', 'wb'))\n",
    "    return vacancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(dct_df):\n",
    "    \"\"\"\n",
    "    Prepare data frame and save results\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dct_df: list\n",
    "        List of parsed json dicts\n",
    "\n",
    "    \"\"\"\n",
    "    # List of columns\n",
    "    df_cols = ['Id',\n",
    "               'Employer',\n",
    "               'Name',\n",
    "               'Salary',\n",
    "               'From',\n",
    "               'To',\n",
    "               'Experience',\n",
    "               'Schedule',\n",
    "               'Keys',\n",
    "               'Description',\n",
    "               ]\n",
    "    # Create pandas dataframe\n",
    "    df = pd.DataFrame(data=dct_df, columns=df_cols)\n",
    "    # Print some info from data frame\n",
    "#     print(\n",
    "#         df[\n",
    "#             df['Salary']\n",
    "#         ]\n",
    "#         [\n",
    "#             ['Employer', 'From', 'To', 'Experience', 'Schedule']\n",
    "#         ][0:10]\n",
    "#     )\n",
    "    # Save to file\n",
    "    df.to_csv(r'hh_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_df():\n",
    "    \"\"\"\n",
    "    Load data frame and analyze results\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sns.set()\n",
    "    print('\\n\\nLoad table and analyze results:')\n",
    "    df = pd.read_csv('hh_data.csv')\n",
    "    print(df[df['Salary']][0:7])\n",
    "\n",
    "    print('\\nNumber of vacancies: {}'.format(df['Id'].count()))\n",
    "    print('\\nVacancy with max salary: ')\n",
    "    print(df.iloc[df[['From', 'To']].idxmax()])\n",
    "    print('\\nVacancy with min salary: ')\n",
    "    print(df.iloc[df[['From', 'To']].idxmin()])\n",
    "\n",
    "    print('\\nDescribe salary data frame: ')\n",
    "    df_stat = df[['From', 'To']].describe().applymap(np.int32)\n",
    "    print(df_stat.iloc[list(range(4)) + [-1]])\n",
    "\n",
    "    print('\\nAverage statistics (average filter for \"From\"-\"To\" parameters):')\n",
    "    comb_ft = np.nanmean(df[df['Salary']][['From', 'To']].to_numpy(), axis=1)\n",
    "    print('Describe salary series:')\n",
    "    print('Min    : %d' % np.min(comb_ft))\n",
    "    print('Max    : %d' % np.max(comb_ft))\n",
    "    print('Mean   : %d' % np.mean(comb_ft))\n",
    "    print('Median : %d' % np.median(comb_ft))\n",
    "\n",
    "    print('\\nMost frequently used words [Keywords]:')\n",
    "    # Collect keys from df\n",
    "    keys_df = df['Keys'].to_list()\n",
    "    # Create a list of keys for all vacancies\n",
    "    lst_keys = []\n",
    "    for keys_elem in keys_df:\n",
    "        for el in keys_elem[1:-1].split(', '):\n",
    "            if el != '':\n",
    "                lst_keys.append(re.sub('\\'', '', el.lower()))\n",
    "    # Unique keys and their counter\n",
    "    set_keys = set(lst_keys)\n",
    "    # Dict: {Key: Count}\n",
    "    dct_keys = {el: lst_keys.count(el) for el in set_keys}\n",
    "    # Sorted dict\n",
    "    srt_keys = dict(sorted(dct_keys.items(), key=lambda x: x[1], reverse=True))\n",
    "    # Return pandas series\n",
    "    most_keys = pd.Series(srt_keys, name='Keys')\n",
    "    print(most_keys[:12])\n",
    "\n",
    "    print('\\nMost frequently used words [Description]:')\n",
    "    # Collect keys from df\n",
    "    words_df = df['Description'].to_list()\n",
    "    # Long string - combine descriptions\n",
    "    words_ls = ' '.join(\n",
    "        [re.sub(' +',\n",
    "                ' ',\n",
    "                re.sub(r'\\d+', '', el.strip().lower())) for el in words_df]\n",
    "    )\n",
    "    # Find all words\n",
    "    words_re = re.findall('[a-zA-Z]+', words_ls)\n",
    "    # Filter words with length < 3\n",
    "    words_l2 = [el for el in words_re if len(el) > 2]\n",
    "    # Unique words\n",
    "    words_st = set(words_l2)\n",
    "    # Remove 'stop words'\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    # XOR for dictionary\n",
    "    words_st ^= stop_words\n",
    "    words_st ^= {'amp', 'quot'}\n",
    "    # Dictionary - {Word: Counter}\n",
    "    words_cnt = {el: words_l2.count(el) for el in words_st}\n",
    "    # Pandas series\n",
    "    most_words = pd.Series(\n",
    "        dict(sorted(words_cnt.items(), key=lambda x: x[1], reverse=True))\n",
    "    )\n",
    "    print(most_words[:12])\n",
    "\n",
    "    print('\\nPlot results. Close figure box to continue...')\n",
    "    fz = plt.figure('Salary plots', figsize=(12, 8), dpi=100)\n",
    "    fz.add_subplot(2, 2, 1)\n",
    "    plt.title('From / To: Boxplot')\n",
    "    sns.boxplot(data=df[['From', 'To']].dropna(), width=0.4)\n",
    "    fz.add_subplot(2, 2, 2)\n",
    "    plt.title('From / To: Swarmplot')\n",
    "    sns.swarmplot(data=df[['From', 'To']].dropna(), size=6)\n",
    "\n",
    "    fz.add_subplot(2, 2, 3)\n",
    "    plt.title('From: Distribution ')\n",
    "    sns.distplot(df['From'].dropna(), bins=12, color='C0')\n",
    "    plt.grid(False)\n",
    "    plt.xlim([-50000, df['From'].max()])\n",
    "    plt.yticks([], [])\n",
    "\n",
    "    fz.add_subplot(2, 2, 4)\n",
    "    plt.title('To: Distribution')\n",
    "    sns.distplot(df['To'].dropna(), bins=12, color='C1')\n",
    "    plt.grid(False)\n",
    "    plt.xlim([-50000, df['To'].max()])\n",
    "    plt.yticks([], [])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(search):\n",
    "    \"\"\"\n",
    "    Main function - combine all methods together\n",
    "\n",
    "    \"\"\"\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('query', help='Search query (e.g. \"Machine learning\")')\n",
    "#     parser.add_argument(\n",
    "#         '--refresh',\n",
    "#         help='Refresh cached data from HH API',\n",
    "#         action='store_true',\n",
    "#         default=False,\n",
    "#     )\n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "    \n",
    "    update_exchange_rates()\n",
    "    print('Collect data from JSON. Create list of vacancies...')\n",
    "    vac_list = get_vacancies(search)\n",
    "    print('Prepare data frame...')\n",
    "    prepare_df(vac_list)\n",
    "    \n",
    "    df = pd.read_csv(\"hh_data.csv\")\n",
    "    df.rename(columns={'Id': 'Link'}, inplace=True)\n",
    "    for i in range(len(df['Link'])):\n",
    "#     print(df['Id'][i])\n",
    "        df['Link'][i] = 'https://kazan.hh.ru/vacancy/' + str(df['Link'][i])\n",
    "    \n",
    "#     return df\n",
    "    df.to_excel('hh_v.xlsx', sheet_name='Angular')\n",
    "    \n",
    "#     analyze_df()\n",
    "#     print('Done! Exit()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try to get rates from URL...\n",
      "Get exchange rates: {'USD': 0.012683, 'EUR': 0.011685, 'RUR': 1}\n",
      "Collect data from JSON. Create list of vacancies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 64/64 [00:08<00:00,  7.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare data frame...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "run('Angular developer')\n",
    "# ndf.to_excel('hh_vacancies.xlsx', sheet_name= 'Java', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parse html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "\n",
    "def get_html(id):\n",
    "    url = 'https://vk.com/id' + id\n",
    "    r = requests.get(url)\n",
    "    text = r.text.encode('cp1251')\n",
    "    html_page = BeautifulSoup(text)\n",
    "#     print(html_page)\n",
    "    return html_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "search = ['java', 'QA', 'angular', 'c#']\n",
    "with pd.ExcelWriter('hh_vacancies.xlsx', engine='openpyxl') as writer:\n",
    "    for el in search:\n",
    "        ndf = run(el)\n",
    "#         arr.append(ndf)\n",
    "        ndf.to_excel(writer, sheet_name=el, index=False)\n",
    "    \n",
    "# print(len(arr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
